\Opensolutionfile{solution_file}[solutions/sols_000]
% в квадратных скобках фактическое имя файла

\chapter{Неразобранные :)}

\section{Джини/Энтропия/ROC}

Распределения максимизирующие энтропию? что-то про ROC кривые до кучи?

\begin{problem}
Для случайных величин  $X$ и $Y$ найдите индекс Джини и энтропию


\begin{tabular}{ccc}
\toprule
$X$ & $0$ & $1$ \\
$\P()$ & $0.2$ & $0.8$ \\
\bottomrule
\end{tabular},
\begin{tabular}{cccc}
\toprule
$Y$ & $0$ & $1$ & $5$ \\
$\P()$ & $0.2$ & $0.3$ & $0.5$ \\
\bottomrule
\end{tabular}


\begin{sol}
\end{sol}
\end{problem}


\begin{problem}
Случайная величина $X$ принимает значение $1$ с вероятностью $p$ и значение $0$ с вероятностью $1-p$.
\begin{enumerate}
\item Постройте график зависимости индекса Джини и энтропии от $p$.
\item Являются ли функции монотонными? выпуклыми?
\item При каком $p$ энтропия и индекс Джини будут максимальны?
\end{enumerate}


\begin{sol}
$I = 2p(1-p)$, энтропия и индекс Джини максимальны при $p=0.5$.
\end{sol}
\end{problem}

\section{Одиноко стоящий дуб}


Типичное заданичие: Вырастить дерево согласно такому-то критерию. Сюда борьбу с NA. Сюда же регуляризацию? Или отдельно?

\begin{problem}
Кот Леопольд анкетировал 20 мышей по трём вопросам: $x$ — «Одобряете ли Вы непримиримую к котам позицию Белого и Серого?», $y$ — «Известно ли Вам куда пропала моя любимая кошка Мурка?» и $z$ — «Известны ли Вам настоящие имена Белого и Серого?» Результаты опроса в таблице:
<<results="asis">>=
set.seed(1975)
x <- sample(c("yes", "no"), size = 20, rep = TRUE)
y <- sample(c("yes", "no"), size = 20, rep = TRUE)
z <- sample(c("yes", "no"), size = 20, rep = TRUE)
xtable(data.frame(x, y, z))
@


\begin{enumerate}
\item Какой фактор нужно использовать при прогнозировании $y$, чтобы минимизировать энтропию?
\item Какой фактор нужно использовать при прогнозировании $y$, чтобы минимизировать индекс Джини?
\end{enumerate}


\begin{sol}
\end{sol}
\end{problem}





\begin{problem}
Постройте регрессионное дерево для набора данных:

\begin{tabular}{cc}
\toprule
$y_i$ & $x_i$ \\
\midrule
$5$ & $0$ \\
$6$ & $1$ \\
$4$ & $2$ \\
$100$ & $3$ \\
\bottomrule
\end{tabular}

Критерий деления узла на два — минимизация $RSS$. Дерево строится до трёх терминальных узлов.


\begin{sol}
\end{sol}
\end{problem}

\begin{problem}
Постройте регрессионное дерево для набора данных:

\begin{tabular}{cc}
\toprule
$y_i$ & $x_i$ \\
\midrule
$100$ & $1$ \\
$102$ & $2$ \\
$103$ & $3$ \\
$50$ & $4$ \\
$55$ & $5$ \\
$61$ & $6$ \\
$70$ & $7$ \\
\bottomrule
\end{tabular}

Критерий деления узла на два — минимизация $RSS$. Узлы делятся до тех пор, пока в узле остаётся больше двух наблюдений.
\begin{sol}
\end{sol}
\end{problem}




\begin{problem}
Дон-Жуан предпочитает брюнеток. Перед Новым Годом он посчитал, что в записной книжке у него 20 блондинок, 40 брюнеток, две рыжих и восемь шатенок. С Нового Года Дон-Жуан решил перенести все сведения в две записные книжки, в одну — брюнеток, во вторую — остальных.

Как изменились индекс Джини и энтропия в результате такого разбиения?
\begin{sol}

\end{sol}
\end{problem}



\begin{problem}
Машка пять дней подряд гадала на ромашке, а затем выкладывала очередную фотку «Машка с ромашкой» в инстаграмчик. Результат гадания — переменная $y_i$, количество лайков у фотки — переменная $x_i$. Постройте классификационное дерево.

\begin{tabular}{cc}
$y_i$ & $x_i$ \\
\hline
плюнет & $10$ \\
поцелует & $11$ \\
поцелует & $12$ \\
к сердцу прижмёт & $13$ \\
к сердцу прижмёт & $14$ \\
\end{tabular}

Дерево строится до идеальной классификации. Критерий деления узла на два — максимальное падение индекса Джини.


\begin{sol}
\end{sol}
\end{problem}




\begin{problem}
У Винни-Пуха есть 100 песенок (кричалок, вопелок, пыхтелок и сопелок). Каждый день он выбирает и поёт одну из них равновероятно наугад. Одну и ту же песенку он может петь несколько раз. Сколько в среднем песенок оказываются неспетыми за 100 дней?


\begin{sol}
$100\cdot \left(\frac{99}{100} \right)^{100}\approx 100/e \approx 37$
\end{sol}
\end{problem}


\begin{problem}
По данной диаграмме рассеяния постройте классификационное дерево для зависимой переменной $y$:
<<"tree_scatter_data", include=FALSE>>=
set.seed(42)
df <- data.frame(x = runif(400), z = runif(400))
df$y <- factor(ifelse( df$x > 0.25 & df$z > 0.5, "yes", "no"))
tikz("../R_plots/tree_scatter_data.tikz", standAlone = FALSE, bareBones = TRUE)
qplot(data = df, x = x, y = z, col = y, shape = y) +
    geom_vline(xintercept = 0.25) + geom_hline(yintercept = 0.5)
invisible(dev.off())
@

<<eval=FALSE>>=
set.seed(42)
df <- data.frame(x = runif(400), z = runif(400))
df$y <- factor(ifelse( df$x > 0.25 & df$z > 0.5, "yes", "no"))
qplot(data = df, x = x, y = z, col = y, shape = y) +
    geom_vline(xintercept = 0.25) + geom_hline(yintercept = 0.5)
@


\begin{figure}
\begin{tikzpicture}[scale = 0.025]
\input{R_plots/tree_scatter_data.tikz}
\end{tikzpicture}
\end{figure}

Дерево необходимо построить до идеальной классификации, в качестве критерия деления узла на два используйте минимизацию индекса Джини.



\begin{sol}
Сначала делим по $z$, потом по $x$, так как индекс Джини в таком порядке падает сильнее.
\end{sol}
\end{problem}


\begin{problem}
Рассмотрим табличку:

\begin{tabular}{ccc}
\toprule
$y_i$ & $x_i$ & $z_i$ \\
\midrule
$y_1$ & $1$ & $2$ \\
$y_2$ & $1$ & $2$ \\
$y_3$ & $2$ & $2$ \\
$y_4$ & $2$ & $1$\\
$y_5$ & $2$ & $1$ \\
$y_6$ & $2$ & $1$ \\
$y_7$ & $2$ & $1$ \\
\bottomrule
\end{tabular}

Сколько существует принципиально разных классификационных деревьев для данного набора данных?
\begin{sol}

\end{sol}
\end{problem}



\begin{problem}
Исследовательница Мишель строит классификационное дерево для бинарной переменной $y_i$. Может ли при разбиении узла на два расти индекс Джини? Энтропия?
\begin{sol}
Нет, в силу выпуклости функций.
\end{sol}
\end{problem}

\begin{problem}
Приведите примеры наборов данных, для которых индекс Джини равен $0$, $0.5$ и $0.999$.
\begin{sol}
Все $y_i$ одинаковые; поровну $y_i$ двух типов; 1000 разных типов $y_i$, по одному наблюдению каждого типа.
\end{sol}
\end{problem}


\begin{problem}
Рассмотрим задачу построения классификационного дерева для бинарной переменной $y_i$. Приведите пример такого набора данных, что никакое разбиения стартового узла на два не снижает индекс Джини, однако двух разбиений достаточно, чтобы снизить индекс Джини до нуля.
\begin{sol}
\begin{tabular}{ccc}
\toprule
$y_i$ & $x_i$ & $z_i$ \\
\midrule
$1$ & $1$ & $1$ \\
$1$ & $2$ & $2$ \\
$0$ & $1$ & $2$ \\
$0$ & $2$ & $1$\\
\bottomrule
\end{tabular}
\end{sol}
\end{problem}


\begin{problem}
Пятачок собрал данные о визитах Винни-Пуха в гости к Кролику. Здесь $x_i$ — количество съеденого мёда в горшках, а $y_i$ — бинарная переменная, отражающая застревание Винни-Пуха при выходе.

Для построения предиктивной модели Пятачок собирается использовать дерево с заданной структурой:

\begin{minipage}{\textwidth}
\begin{minipage}[t]{0.4\textwidth}
\centering
\begin{tabular}{cc}
\toprule
$y_i$ & $x_i$ \\
\midrule
0 & 1 \\
1 & 4 \\
1 & 2 \\
0 & 3 \\
1 & 3 \\
0 & 1 \\
\bottomrule
\end{tabular}
\end{minipage}
\hfill
\begin{minipage}{0.4\textwidth}
\begin{forest}
  [$x>3.5$, circle, draw
    [$w_L$, circle, draw]
    [$x>2.5$, circle, draw
       [$w_{RL}$, circle, draw]
       [$w_{RR}$, circle, draw]
    ]
  ]
\end{forest}
\end{minipage}
\end{minipage}

Пятачок использует квадратичную аппроксимацию для логистической функции потерь:
\[
Obj(w) = \sum_{i=1}^n \left(loss(y_i, 0) +
loss'_{w}(y_i, 0) (w_i - 0) +
\frac{1}{2} loss''_{ww}(y_i, 0) (w_i - 0)^2 \right) + \frac{1}{2} \lambda |w|^2.
\]


Помогите Очень Маленькому Существу подобрать оптимальные веса $(w_i)$ при $\lambda = 1$.
\begin{sol}
\end{sol}
\end{problem}


\section{Логистическая функция}

Логистическое распределение? Перевод y=0/1 в y=-1/1. Максимум правдоподобия в минимум штрафа? Предельные эффекты?

\begin{problem}
Рассмотрим логистическую функцию $\Lambda(w) = e^w / (1 + e^w)$.
\begin{enumerate}
\item Как связаны между собой $\Lambda(w)$ и  $\Lambda(-w)$?
\item Как связаны между собой $\Lambda'(w)$ и  $\Lambda'(-w)$?
\item Постройте графики функций $\Lambda(w)$ и $\Lambda'(w)$.
\item Найдите $\Lambda(0)$, $\Lambda'(0)$, $\ln\Lambda(0)$.
\item Найдите обратную функцию $\Lambda^{-1}(p)$.
\item Как связаны между собой $\frac{d\ln\Lambda(w)}{dw}$ и $\Lambda(-w)$?
\item Как связаны между собой $\frac{d\ln\Lambda(-w)}{dw}$ и $\Lambda(w)$?
\item Разложите $h(\beta_1, \beta_2)=\ln\Lambda(y_i(\beta_1 + \beta_2 x_i))$ в ряд Тейлора до второго порядка в окрестности точки $\beta_1=0$, $\beta_2=0$.
\end{enumerate}

   .
\begin{sol}
  $1$ и $0$.
\end{sol}
\end{problem}


\begin{problem}
Исследовательница Октябрина пытается предсказать, купит ли покупатель слона. Октябрина предполагает, что у каждого покупателя есть ненаблюдаемая полезность от покупки слона, $y_i^*$, складывающаяся из величины $w_i$, зависящей от характеристик покупателя, и случайной составляющей $u_i$:
\[
y_i^* = w_i + u_i, \quad u_i \sim Logistic
\]
Покупка слона, $y_i$, ($1$ — купит, $0$ — не купит) однозначно определяется полезностью покупки:
\[
y_i = \begin{cases}
1, \text{ если } y_i^* \geq 0 \\
1, \text{ если } y_i^* < 0 \\
\end{cases}
\]

\begin{enumerate}
\item Выпишите логарифмическую функцию правдоподобия и функцию потерь при известных $w_i$.
\item Как изменится ответ, если факт покупки слона будет кодироваться по-другому: $1$ — купит, $(-1)$ — не купит?
\item Разложите функцию потерь в ряд Тейлора до второго члена в окрестности точки $w_0=(w_{01}, w_{02}, \ldots, w_{0n})$.
\end{enumerate}
\begin{sol}
\end{sol}
\end{problem}

\begin{problem}
Нарисовано дерево: деление 1, справа от первого деления — деление 2. Веса равны $w_L$, $w_{RL}$, $w_{LL}$. Дана выборка.

\begin{enumerate}
\item Выпишите в явном виде функцию правдоподобия и логистическую функцию потерь.
\item Оцените $w$ методом максимального правдоподобия.
\item Тут другую функцию потерь написать!
\item Разложите функцию потерь в окрестности $w = (0, 0, 1)$ в ряд Тейлора до второго члена и примерно оцените $w$.
\end{enumerate}

\begin{sol}
\end{sol}
\end{problem}





\section{Мини-мими-лес}

Типичное: Два-три дерева. По ним построить прогноз/оценить важность переменных. Что еще?



\section{Регуляризация}

Общая идея. Парадокс James-Stein. Для среднего, для регрессии, для дерева. L1 и L2.



\section{кросс-валидацию}


\begin{problem}
Вася измерил вес трёх покемонов, $y_1=5$, $y_2=10$, $y_3=15$. Вася хочет спрогнозировать вес следующего покемона. Модель для веса покемонов у Васи очень простая, $y_i = \mu + u_i$, поэтому прогнозирует Вася по формуле $\hat y_i = \hat\mu$.

В результате Вася использует следующую целевую функцию:
\[
\sum (y_i - \hat \mu)^2 + \lambda \cdot \hat \mu^2
\]

\begin{enumerate}
  \item Найдите оптимальное $\hat\mu$ при $\lambda =0$.
  \item Найдите оптимальное $\hat\mu$ при произвольном $\lambda$.
  \item Подберите оптимальное $\lambda$ с помощью кросс-валидации «выкинь одного».
  \item Найдите оптимальное $\hat\mu$ при $\lambda_{CV}$.
\end{enumerate}


\begin{sol}
\end{sol}
\end{problem}


Как это делать руками? Какие тут теоретические задачи?

Упр: Дано одно-два-три дерева. И 5 наблюдений. Посчитать кросс-валидационную ошибку.

Упр: На наборе данных в 5 наблюдений подобрать параметр жесткости с помощью кросс-валидации.


\section{На природу!}

Упр: сделайте с дефолтными параметрами и ответьте на все подробности про алгоритм

тут решения в python/R.

Упр: Нарисуйте дерево номер 5.



\Closesolutionfile{solution_file}
